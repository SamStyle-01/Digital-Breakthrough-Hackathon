{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcOTrFGMMSrR",
        "outputId": "930df587-319a-4a80-fd5c-82ccc3cd066a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.32.0\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl.metadata (118 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/118.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m112.6/118.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.32.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.32.0) (2024.8.30)\n",
            "Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.32.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.0/408.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Базовые библиотеки\n",
        "!pip install --quiet torch\n",
        "!pip install transformers==4.32.0\n",
        "!pip install --quiet accelerate\n",
        "!pip install --quiet bitsandbytes\n",
        "# Библиотеки для обработки текста\n",
        "!pip install --quiet sentencepiece\n",
        "!pip install --quiet sentence-transformers\n",
        "!pip install --quiet einops\n",
        "!pip install --quiet xformers\n",
        "# Langchain и зависимости\n",
        "!pip install --quiet langchain\n",
        "!pip install --quiet langchain-community\n",
        "!pip install --quiet huggingface_hub\n",
        "# Библиотеки для работы с документами\n",
        "!pip install --quiet pandas\n",
        "!pip install --quiet numpy\n",
        "!pip install --quiet pypdf\n",
        "!pip install --quiet python-docx\n",
        "!pip install --quiet faiss-cpu\n",
        "# Llama\n",
        "!pip install --quiet llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fhxra-HQs_V",
        "outputId": "12c03b7f-a3c3-4a8b-be76-6d368f6cc446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.12-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading PyMuPDF-1.24.12-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.24.12\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Crmn4pkVWSG",
        "outputId": "a53e3648-8685-4d9b-c915-ea4efbeaa47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from tabula-py) (2.2.2)\n",
            "Requirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.10/dist-packages (from tabula-py) (1.26.4)\n",
            "Requirement already satisfied: distro in /usr/lib/python3/dist-packages (from tabula-py) (1.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.16.0)\n",
            "Downloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tabula-py\n",
            "Successfully installed tabula-py-2.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tabula-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY6dQnLwVmmT",
        "outputId": "7f430f19-a2ff-463b-8846-2876dca233d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.4.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzpPaUZZV6Dn",
        "outputId": "187d7825-8687-4404-8d29-a2fa65943366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (10.4.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsXUGGZKetg3",
        "outputId": "17c1f329-332d-4920-d664-a9d7223acfe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6OAFSNhie4C",
        "outputId": "045a73a8-6c8a-4506-9bf7-836032181600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AqQA5Dxevsy",
        "outputId": "ef425089-b566-4760-f42b-0ea84f7c6208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.44.1\n"
          ]
        }
      ],
      "source": [
        "import bitsandbytes as bnb\n",
        "\n",
        "# Проверка версии bitsandbytes\n",
        "print(bnb.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4akPmx7h1el",
        "outputId": "9e2e503d-3741-40ac-e10a-6cfecd0169e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting peft\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.46.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "Successfully installed peft-0.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YLE1z5XiIcN",
        "outputId": "0ac37da4-7fd4-4860-b2b5-e425f1ec66e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qRrn3bqjcY4p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import datetime\n",
        "import re\n",
        "import threading\n",
        "import asyncio\n",
        "import gc\n",
        "import warnings\n",
        "import torch\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from queue import Queue\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import io\n",
        "import hashlib\n",
        "import pickle\n",
        "from PIL import Image\n",
        "\n",
        "import docx\n",
        "import tabula\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from enum import Enum\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Optional, Union, Tuple\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        "    GenerationConfig\n",
        ")\n",
        "from langchain.document_loaders import UnstructuredFileLoader, OnlinePDFLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.schema import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl54WQBhuheP",
        "outputId": "c0be0fc5-4210-4817-8c84-f28748f135f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "35EHv67j0-Z6"
      },
      "outputs": [],
      "source": [
        "# Базовые библиотеки\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n",
        "\n",
        "# Настраиваем логирование\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('qa_system.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from accelerate import Accelerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw9F4Oauc4EP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "cbd698f2ee5a408aac2f3dc4d8d8fdf9",
            "23bcb18458874792bb0c67a9d4a3e609",
            "48a65cdf783b45b89ee6007b3303baa4",
            "e3aa7117b8c24a8a935df4be99c52a2a",
            "37c593dff5394a659629851e79db995e",
            "568eafc843a14e9982ef66faaa943b87",
            "7d276087fc8d4f1fa95f043e44fff651",
            "94cbdd0ac68940f2bbd43ae93838889b",
            "dbe923df713649afa635c3ca993049b0",
            "66bbd0aee079401c81daa1ed8348b957",
            "6ce6ae18829440879239f5cea7237062"
          ]
        },
        "outputId": "46de5db2-a6be-4a5e-c7a9-1f8f0216041f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:No regulatory documents found!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Eco QA System...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbd698f2ee5a408aac2f3dc4d8d8fdf9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "class BaseAgent:\n",
        "    \"\"\"Базовый класс для всех агентов\"\"\"\n",
        "    def __init__(self, model_name: str = \"gpt2\"):  # Используем gpt2 вместо IlyaGusev/saiga_mistral_7b\n",
        "        self.model_name = model_name\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.llm = None\n",
        "        self.embeddings = None\n",
        "        self.tokenizer = None\n",
        "        self.initialize_model()\n",
        "\n",
        "    def initialize_model(self):\n",
        "        \"\"\"Инициализация модели и эмбеддингов\"\"\"\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.model_name,\n",
        "                token=os.environ[\"HUGGINGFACE_TOKEN\"]\n",
        "            )\n",
        "            self.llm = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                offload_folder=\"/for_model\",\n",
        "                device_map=\"auto\",  # Используем GPU, если доступно\n",
        "                torch_dtype=torch.float16,  # Используем FP16 для уменьшения потребления памяти\n",
        "                token=os.environ[\"HUGGINGFACE_TOKEN\"]\n",
        "            )\n",
        "\n",
        "            # Используем более легкие эмбеддинги для векторного поиска\n",
        "            self.embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error initializing model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def process(self, *args, **kwargs):\n",
        "        \"\"\"Абстрактный метод для обработки\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class BaseDocument:\n",
        "    \"\"\"Базовый класс для работы с документами\"\"\"\n",
        "    def __init__(self, path: str):\n",
        "        self.path = Path(path)\n",
        "        self.content = None\n",
        "        self.metadata = {}\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def load(self) -> bool:\n",
        "        \"\"\"Загрузка документа\"\"\"\n",
        "        try:\n",
        "            if not self.path.exists():\n",
        "                self.logger.error(f\"File not found: {self.path}\")\n",
        "                return False\n",
        "\n",
        "            self.metadata['filename'] = self.path.name\n",
        "            self.metadata['created_at'] = datetime.datetime.now()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading document: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "\n",
        "class PDFDocument(BaseDocument):\n",
        "    \"\"\"Класс для работы с PDF документами\"\"\"\n",
        "    def load(self) -> bool:\n",
        "        if super().load():\n",
        "            try:\n",
        "                loader = PyPDFLoader(str(self.path))\n",
        "                self.content = loader.load()\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error loading PDF: {str(e)}\")\n",
        "                return False\n",
        "\n",
        "class DocxDocument(BaseDocument):\n",
        "    \"\"\"Класс для работы с DOCX документами\"\"\"\n",
        "    def load(self) -> bool:\n",
        "        if super().load():\n",
        "            try:\n",
        "                loader = UnstructuredFileLoader(str(self.path))\n",
        "                self.content = loader.load()\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error loading DOCX: {str(e)}\")\n",
        "                return False\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Класс для обработки документов\"\"\"\n",
        "    def __init__(self):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len\n",
        "        )\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def process_document(self, document: BaseDocument) -> List[Document]:\n",
        "        \"\"\"Обработка документа и разбиение на чанки\"\"\"\n",
        "        try:\n",
        "            if document.content:\n",
        "                return self.text_splitter.split_documents(document.content)\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing document: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class EnhancedDocumentProcessor:\n",
        "    \"\"\"Расширенный процессор документов с поддержкой таблиц\"\"\"\n",
        "    def __init__(self):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "        self.table_processor = TableProcessor()\n",
        "        self.table_extractor = TableExtractor()\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "        # Кэш для обработанных документов\n",
        "        self.document_cache = {}\n",
        "        self.cache_lock = threading.Lock()\n",
        "\n",
        "    def get_cache_key(self, document: BaseDocument) -> str:\n",
        "        \"\"\"Генерация ключа кэша для документа\"\"\"\n",
        "        return f\"{document.path}_{document.metadata.get('last_modified', '')}\"\n",
        "\n",
        "    def process_document(self, document: BaseDocument) -> List[Document]:\n",
        "        \"\"\"Комплексная обработка документа включая текст и таблицы\"\"\"\n",
        "        try:\n",
        "            # Проверяем кэш\n",
        "            cache_key = self.get_cache_key(document)\n",
        "            with self.cache_lock:\n",
        "                if cache_key in self.document_cache:\n",
        "                    self.logger.info(f\"Using cached version for {document.path}\")\n",
        "                    return self.document_cache[cache_key]\n",
        "\n",
        "            all_documents = []\n",
        "\n",
        "            # Обработка текстового содержимого\n",
        "            if document.content:\n",
        "                text_documents = self.process_text_content(document)\n",
        "                all_documents.extend(text_documents)\n",
        "\n",
        "            # Обработка таблиц в зависимости от типа документа\n",
        "            table_documents = self.process_tables(document)\n",
        "            all_documents.extend(table_documents)\n",
        "\n",
        "            # Сохраняем в кэш\n",
        "            with self.cache_lock:\n",
        "                self.document_cache[cache_key] = all_documents\n",
        "\n",
        "            return all_documents\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing document {document.path}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def process_text_content(self, document: BaseDocument) -> List[Document]:\n",
        "        \"\"\"Обработка текстового содержимого документа\"\"\"\n",
        "        try:\n",
        "            if not document.content:\n",
        "                return []\n",
        "\n",
        "            # Разбиваем текст на чанки\n",
        "            chunks = self.text_splitter.split_documents(document.content)\n",
        "\n",
        "            # Добавляем дополнительные метаданные\n",
        "            for chunk in chunks:\n",
        "                chunk.metadata.update({\n",
        "                    \"source\": str(document.path),\n",
        "                    \"type\": \"text\",\n",
        "                    \"document_type\": document.__class__.__name__\n",
        "                })\n",
        "\n",
        "            return chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing text content: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def process_tables(self, document: BaseDocument) -> List[Document]:\n",
        "        \"\"\"Обработка таблиц в документе\"\"\"\n",
        "        try:\n",
        "            if isinstance(document, PDFDocument):\n",
        "                return self.table_extractor.extract_from_pdf(document.path)\n",
        "            elif isinstance(document, DocxDocument):\n",
        "                return self.table_extractor.extract_from_docx(document.path)\n",
        "            elif isinstance(document, ExcelDocument):  # Нужно добавить этот класс\n",
        "                return self.table_extractor.extract_from_excel(document.path)\n",
        "            return []\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing tables: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def extract_table_context(self, table_documents: List[Document], window_size: int = 2) -> Dict[int, str]:\n",
        "        \"\"\"Извлечение контекста вокруг таблиц\"\"\"\n",
        "        table_contexts = {}\n",
        "\n",
        "        for doc in table_documents:\n",
        "            if doc.metadata.get(\"type\") == \"table_description\":\n",
        "                page = doc.metadata.get(\"page\", 0)\n",
        "                # Ищем ближайший текст до и после таблицы\n",
        "                context_docs = [\n",
        "                    d for d in self.document_cache.get(doc.metadata[\"source\"], [])\n",
        "                    if d.metadata.get(\"type\") == \"text\" and\n",
        "                    abs(d.metadata.get(\"page\", 0) - page) <= window_size\n",
        "                ]\n",
        "\n",
        "                context = \"\\n\".join([d.page_content for d in context_docs])\n",
        "                table_contexts[page] = context\n",
        "\n",
        "        return table_contexts\n",
        "\n",
        "    def batch_process_documents(self, documents: List[BaseDocument], max_workers: int = 4) -> List[Document]:\n",
        "        \"\"\"Параллельная обработка документов\"\"\"\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "                # Запускаем обработку документов параллельно\n",
        "                future_to_doc = {\n",
        "                    executor.submit(self.process_document, doc): doc\n",
        "                    for doc in documents\n",
        "                }\n",
        "\n",
        "                all_documents = []\n",
        "                for future in future_to_doc:\n",
        "                    try:\n",
        "                        documents = future.result()\n",
        "                        all_documents.extend(documents)\n",
        "                    except Exception as e:\n",
        "                        self.logger.error(f\"Error in batch processing: {str(e)}\")\n",
        "\n",
        "                return all_documents\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in batch processing: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def analyze_tables(self, documents: List[Document]) -> Dict[str, Any]:\n",
        "        \"\"\"Анализ всех таблиц в документах\"\"\"\n",
        "        try:\n",
        "            table_stats = {\n",
        "                \"total_tables\": 0,\n",
        "                \"tables_by_type\": {},\n",
        "                \"tables_by_document\": {},\n",
        "                \"common_units\": set(),\n",
        "                \"value_ranges\": {}\n",
        "            }\n",
        "\n",
        "            for doc in documents:\n",
        "                if doc.metadata.get(\"type\") == \"table_description\":\n",
        "                    table_stats[\"total_tables\"] += 1\n",
        "\n",
        "                    # Подсчет таблиц по типам\n",
        "                    table_type = doc.metadata.get(\"table_type\", \"unknown\")\n",
        "                    table_stats[\"tables_by_type\"][table_type] = \\\n",
        "                        table_stats[\"tables_by_type\"].get(table_type, 0) + 1\n",
        "\n",
        "                    # Подсчет таблиц по документам\n",
        "                    source = doc.metadata.get(\"source\", \"unknown\")\n",
        "                    table_stats[\"tables_by_document\"][source] = \\\n",
        "                        table_stats[\"tables_by_document\"].get(source, 0) + 1\n",
        "\n",
        "                    # Анализ содержимого таблицы\n",
        "                    content = doc.page_content\n",
        "                    # Извлекаем единицы измерения\n",
        "                    units = re.findall(r'\\((.*?)\\)|\\[(.*?)\\]', content)\n",
        "                    table_stats[\"common_units\"].update([u[0] or u[1] for u in units if any(u)])\n",
        "\n",
        "            return table_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error analyzing tables: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "class QuestionCategory(Enum):\n",
        "    \"\"\"Категории вопросов на основе анализа тренировочных данных\"\"\"\n",
        "    INVENTORY = \"inventory\"  # Вопросы по инвентаризации\n",
        "    TECHNICAL = \"technical\"  # Технические вопросы по процессам\n",
        "    REGULATORY = \"regulatory\"  # Вопросы по нормативам\n",
        "    EMISSIONS = \"emissions\"  # Вопросы по выбросам\n",
        "    COMPANY_SPECIFIC = \"company_specific\"  # Вопросы по конкретному предприятию\n",
        "    GENERAL = \"general\"  # Общие вопросы\n",
        "\n",
        "@dataclass\n",
        "class TrainingExample:\n",
        "    \"\"\"Структура обучающего примера\"\"\"\n",
        "    question: str\n",
        "    answer: str\n",
        "    category: QuestionCategory\n",
        "    document_reference: Optional[str] = None\n",
        "\n",
        "class QAPromptManager:\n",
        "    \"\"\"Менеджер промптов с few-shot обучением\"\"\"\n",
        "\n",
        "    def __init__(self, training_data_path: str):\n",
        "        self.training_examples = self._load_training_data(training_data_path)\n",
        "        self.category_templates = self._initialize_templates()\n",
        "\n",
        "    def _load_training_data(self, path: str) -> List[TrainingExample]:\n",
        "        \"\"\"Загрузка тренировочных данных\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(path, delimiter='\\t')\n",
        "            examples = []\n",
        "\n",
        "            for _, row in df.iterrows():\n",
        "                # Определяем категорию вопроса\n",
        "                category = self._categorize_question(row['Вопрос'])\n",
        "\n",
        "                examples.append(TrainingExample(\n",
        "                    question=row['Вопрос'],\n",
        "                    answer=row['Ответ'],\n",
        "                    category=category,\n",
        "                    document_reference=row.get('Документ', None)\n",
        "                ))\n",
        "\n",
        "            return examples\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading training data: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _categorize_question(self, question: str) -> QuestionCategory:\n",
        "        \"\"\"Определение категории вопроса\"\"\"\n",
        "        question = question.lower()\n",
        "\n",
        "        # Паттерны для определения категории\n",
        "        patterns = {\n",
        "            QuestionCategory.INVENTORY: r'инвентаризац|источник|выброс',\n",
        "            QuestionCategory.TECHNICAL: r'процесс|оборудован|техник|спецтехник',\n",
        "            QuestionCategory.REGULATORY: r'требован|норматив|сзз|пдк|предельн',\n",
        "            QuestionCategory.EMISSIONS: r'выброс|загрязня|очистк|газ',\n",
        "            QuestionCategory.COMPANY_SPECIFIC: r'предприяти|ао|площадк'\n",
        "        }\n",
        "\n",
        "        for category, pattern in patterns.items():\n",
        "            if re.search(pattern, question):\n",
        "                return category\n",
        "\n",
        "        return QuestionCategory.GENERAL\n",
        "\n",
        "    def _initialize_templates(self) -> Dict[QuestionCategory, str]:\n",
        "        \"\"\"Инициализация шаблонов промптов для каждой категории\"\"\"\n",
        "        return {\n",
        "            QuestionType.GENERAL: QAPrompt(\n",
        "                system=\"\"\"Ты - эксперт в области экологии и природоохраны. Твоя задача - отвечать на вопросы,\n",
        "                используя предоставленный контекст. Отвечай кратко, но информативно. Если в контексте нет\n",
        "                информации для ответа, скажи об этом. Используй только факты из контекста.\"\"\",\n",
        "                user=\"Вопрос: {question}\\n\\nКонтекст: {context}\\n\\nОтвет:\"\n",
        "            ),\n",
        "\n",
        "            QuestionType.REGULATORY: QAPrompt(\n",
        "                system=\"\"\"Ты - эксперт по экологическому законодательству. Отвечай на вопросы о нормативах,\n",
        "                требованиях и законодательстве. Всегда указывай источник информации из контекста. Будь точным\n",
        "                в формулировках.\"\"\",\n",
        "                user=\"Вопрос о нормативах: {question}\\n\\nКонтекст из документов: {context}\\n\\nОтвет с указанием источника:\"\n",
        "            ),\n",
        "\n",
        "            QuestionType.NUMERICAL: QAPrompt(\n",
        "                system=\"\"\"Ты - аналитик экологических данных. Отвечай на вопросы, требующие числовых расчётов\n",
        "                или анализа количественных показателей. Всегда указывай единицы измерения и источник данных.\"\"\",\n",
        "                user=\"Вопрос о количественных показателях: {question}\\n\\nДанные: {context}\\n\\nОтвет с расчётами:\"\n",
        "            ),\n",
        "\n",
        "            QuestionType.TABLE: QAPrompt(\n",
        "                system=\"\"\"Ты - специалист по анализу табличных данных в экологических документах. Твоя задача -\n",
        "                извлекать информацию из таблиц и отвечать на вопросы по ним. Учитывай единицы измерения и\n",
        "                контекст таблицы.\"\"\",\n",
        "                user=\"Вопрос по таблице: {question}\\n\\nДанные таблицы: {context}\\n\\nЕдиницы измерения: {units}\\n\\nОтвет:\"\n",
        "            ),\n",
        "\n",
        "            QuestionType.COMPARISON: QAPrompt(\n",
        "                system=\"\"\"Ты - эксперт по сравнительному анализу экологических данных. Сравнивай показатели,\n",
        "                учитывая контекст, нормативы и единицы измерения. Выделяй существенные различия и тренды.\"\"\",\n",
        "                user=\"Задача сравнения: {question}\\n\\nДанные для сравнения: {context}\\n\\nОтвет с анализом:\"\n",
        "            ),\n",
        "\n",
        "            QuestionType.TECHNICAL: QAPrompt(\n",
        "                system=\"\"\"Ты - технический специалист по экологическим процессам. Отвечай на вопросы о\n",
        "                технологических процессах, оборудовании и методах измерений. Используй технические термины\n",
        "                корректно.\"\"\",\n",
        "                user=\"Технический вопрос: {question}\\n\\nТехническая документация: {context}\\n\\nОтвет:\"\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def get_examples_for_category(self, category: QuestionCategory, n_examples: int = 3) -> str:\n",
        "        \"\"\"Получение примеров для категории\"\"\"\n",
        "        relevant_examples = [ex for ex in self.training_examples if ex.category == category]\n",
        "        selected_examples = relevant_examples[:n_examples]\n",
        "\n",
        "        examples_text = \"\"\n",
        "        for ex in selected_examples:\n",
        "            examples_text += f\"В: {ex.question}\\nО: {ex.answer}\\n\\n\"\n",
        "\n",
        "        return examples_text\n",
        "\n",
        "    def get_prompt(self, question: str, context: str) -> str:\n",
        "        \"\"\"Получение промпта для вопроса\"\"\"\n",
        "        category = self._categorize_question(question)\n",
        "        template = self.category_templates[category]\n",
        "        examples = self.get_examples_for_category(category)\n",
        "\n",
        "        return template.format(\n",
        "            examples=examples,\n",
        "            question=question,\n",
        "            context=context\n",
        "        )\n",
        "\n",
        "class EnhancedQAAgent(BaseAgent):\n",
        "    \"\"\"Улучшенный QA агент с использованием тренировочных данных\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"IlyaGusev/saiga_mistral_7b\",\n",
        "        training_data_path: str = \"train.csv\",\n",
        "        vector_store: Optional[FAISS] = None,\n",
        "    ):\n",
        "        super().__init__(model_name)\n",
        "        self.prompt_manager = QAPromptManager(training_data_path)\n",
        "        self.vector_store = vector_store\n",
        "\n",
        "    def process_question(\n",
        "        self,\n",
        "        question: str,\n",
        "        additional_context: Optional[str] = None,\n",
        "        max_context_tokens: int = 2000\n",
        "    ) -> str:\n",
        "        \"\"\"Обработка вопроса и генерация ответа\"\"\"\n",
        "        try:\n",
        "            # Получаем релевантный контекст\n",
        "            context = self._get_relevant_context(question, max_context_tokens)\n",
        "            if additional_context:\n",
        "                context = f\"{context}\\n\\nДополнительный контекст:\\n{additional_context}\"\n",
        "\n",
        "            # Получаем промпт с few-shot примерами\n",
        "            prompt = self.prompt_manager.get_prompt(question, context)\n",
        "\n",
        "            # Генерируем ответ\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=4096\n",
        "            ).to(self.llm.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.llm.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=512,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True\n",
        "                )\n",
        "\n",
        "            answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Извлекаем только ответ после последнего \"Ответ:\"\n",
        "            answer = answer.split(\"Ответ:\")[-1].strip()\n",
        "\n",
        "            return self._validate_answer(answer, context)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing question: {str(e)}\")\n",
        "            return f\"Произошла ошибка при обработке вопроса: {str(e)}\"\n",
        "\n",
        "    def _get_relevant_context(self, question: str, max_tokens: int) -> str:\n",
        "        \"\"\"Получение релевантного контекста\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return \"\"\n",
        "\n",
        "        # Получаем похожие документы\n",
        "        docs = self.vector_store.similarity_search(question, k=5)\n",
        "\n",
        "        # Объединяем контекст\n",
        "        context_parts = []\n",
        "        total_tokens = 0\n",
        "\n",
        "        for doc in docs:\n",
        "            tokens = len(self.tokenizer.encode(doc.page_content))\n",
        "            if total_tokens + tokens > max_tokens:\n",
        "                break\n",
        "\n",
        "            context_parts.append(doc.page_content)\n",
        "            total_tokens += tokens\n",
        "\n",
        "        return \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    def _validate_answer(self, answer: str, context: str) -> str:\n",
        "        \"\"\"Валидация ответа\"\"\"\n",
        "        # Проверяем наличие чисел в ответе\n",
        "        numbers_in_answer = re.findall(r'\\d+(?:\\.\\d+)?', answer)\n",
        "        if numbers_in_answer:\n",
        "            # Проверяем, есть ли эти числа в контексте\n",
        "            numbers_in_context = re.findall(r'\\d+(?:\\.\\d+)?', context)\n",
        "            if not all(num in numbers_in_context for num in numbers_in_answer):\n",
        "                return \"Не удалось найти подтверждение числовых данных в контексте\"\n",
        "\n",
        "        # Проверяем минимальную длину ответа\n",
        "        if len(answer.split()) < 3:\n",
        "            return \"Ответ слишком короткий\"\n",
        "\n",
        "        # Проверяем наличие ключевых слов из вопроса\n",
        "        # question_keywords = set(re.findall(r'\\w+', question.lower()))\n",
        "        # answer_keywords = set(re.findall(r'\\w+', answer.lower()))\n",
        "        # if not question_keywords & answer_keywords:\n",
        "        #     return \"Ответ не содержит ключевых слов из вопроса\"\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def batch_process(\n",
        "        self,\n",
        "        questions: List[str],\n",
        "        output_path: Optional[str] = None\n",
        "    ) -> Union[List[str], None]:\n",
        "        \"\"\"Пакетная обработка вопросов\"\"\"\n",
        "        answers = []\n",
        "\n",
        "        for question in tqdm(questions, desc=\"Processing questions\"):\n",
        "            answer = self.process_question(question)\n",
        "            answers.append(answer)\n",
        "\n",
        "        # Сохраняем результаты\n",
        "        if output_path:\n",
        "            df = pd.DataFrame({\n",
        "                \"question\": questions,\n",
        "                \"answer\": answers\n",
        "            })\n",
        "            df.to_csv(output_path, index=False)\n",
        "            return None\n",
        "\n",
        "        return answers\n",
        "\n",
        "@dataclass\n",
        "class ProcessingMetadata:\n",
        "    \"\"\"Метаданные обработки документа\"\"\"\n",
        "    document_type: str\n",
        "    processing_date: datetime.datetime\n",
        "    chunks_count: int\n",
        "    tables_count: int\n",
        "    images_count: int\n",
        "    word_count: int\n",
        "    processing_time: float\n",
        "    hash: str\n",
        "\n",
        "class DocumentProcessingAgent(BaseAgent):\n",
        "    \"\"\"Агент для обработки и подготовки документов\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"IlyaGusev/saiga_mistral_7b\",\n",
        "        cache_dir: Optional[str] = \"./cache\",\n",
        "        enable_ocr: bool = True,\n",
        "        chunk_size: int = 1000,\n",
        "        chunk_overlap: int = 200,\n",
        "        batch_size: int = 10\n",
        "    ):\n",
        "        super().__init__(model_name)\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.enable_ocr = enable_ocr\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Инициализация компонентов\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        self.table_processor = TableProcessor()\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def _calculate_hash(self, file_path: Path) -> str:\n",
        "        \"\"\"Вычисление хеша файла\"\"\"\n",
        "        hasher = hashlib.sha256()\n",
        "        with open(file_path, 'rb') as f:\n",
        "            for chunk in iter(lambda: f.read(4096), b''):\n",
        "                hasher.update(chunk)\n",
        "        return hasher.hexdigest()\n",
        "\n",
        "    def _get_cache_path(self, file_hash: str) -> Path:\n",
        "        \"\"\"Получение пути к кэшу для файла\"\"\"\n",
        "        return self.cache_dir / f\"{file_hash}.pickle\"\n",
        "\n",
        "    def _load_from_cache(self, file_hash: str) -> Optional[Tuple[List[Document], ProcessingMetadata]]:\n",
        "        \"\"\"Загрузка обработанного документа из кэша\"\"\"\n",
        "        cache_path = self._get_cache_path(file_hash)\n",
        "        if cache_path.exists():\n",
        "            try:\n",
        "                with open(cache_path, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error loading from cache: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "    def _save_to_cache(self, file_hash: str, data: Tuple[List[Document], ProcessingMetadata]):\n",
        "        \"\"\"Сохранение обработанного документа в кэш\"\"\"\n",
        "        cache_path = self._get_cache_path(file_hash)\n",
        "        try:\n",
        "            with open(cache_path, 'wb') as f:\n",
        "                pickle.dump(data, f)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving to cache: {str(e)}\")\n",
        "\n",
        "    def _extract_text_from_docx(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"Извлечение текста из DOCX документа\"\"\"\n",
        "        documents = []\n",
        "        try:\n",
        "            doc = docx.Document(file_path)\n",
        "\n",
        "            # Обработка параграфов\n",
        "            for i, para in enumerate(doc.paragraphs):\n",
        "                if para.text.strip():\n",
        "                    documents.append(\n",
        "                        Document(\n",
        "                            page_content=para.text,\n",
        "                            metadata={\n",
        "                                \"source\": str(file_path),\n",
        "                                \"type\": \"paragraph\",\n",
        "                                \"index\": i\n",
        "                            }\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            # Обработка таблиц\n",
        "            for i, table in enumerate(doc.tables):\n",
        "                table_data = []\n",
        "                for row in table.rows:\n",
        "                    row_data = [cell.text for cell in row.cells]\n",
        "                    table_data.append(row_data)\n",
        "\n",
        "                if table_data:\n",
        "                    df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
        "                    documents.extend(\n",
        "                        self.table_processor.process_table(\n",
        "                            df,\n",
        "                            str(file_path),\n",
        "                            table_number=i\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            return documents\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting text from DOCX: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def _extract_text_from_pdf(self, file_path: Path) -> List[Document]:\n",
        "        \"\"\"Извлечение текста из PDF документа\"\"\"\n",
        "        documents = []\n",
        "        try:\n",
        "            pdf_document = fitz.open(file_path)\n",
        "\n",
        "            for page_num in range(pdf_document.page_count):\n",
        "                page = pdf_document[page_num]\n",
        "\n",
        "                # Извлечение текста\n",
        "                text = page.get_text()\n",
        "                if text.strip():\n",
        "                    documents.append(\n",
        "                        Document(\n",
        "                            page_content=text,\n",
        "                            metadata={\n",
        "                                \"source\": str(file_path),\n",
        "                                \"type\": \"text\",\n",
        "                                \"page\": page_num + 1\n",
        "                            }\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                # Извлечение таблиц\n",
        "                tables = page.find_tables()\n",
        "                if tables and tables.tables:\n",
        "                    for i, table in enumerate(tables.tables):\n",
        "                        table_data = []\n",
        "                        for row in table.extract():\n",
        "                            table_data.append([cell.strip() for cell in row])\n",
        "\n",
        "                        if table_data:\n",
        "                            df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
        "                            documents.extend(\n",
        "                                self.table_processor.process_table(\n",
        "                                    df,\n",
        "                                    str(file_path),\n",
        "                                    page_num + 1,\n",
        "                                    table_number=i\n",
        "                                )\n",
        "                            )\n",
        "\n",
        "                # OCR для изображений\n",
        "                if self.enable_ocr:\n",
        "                    images = page.get_images()\n",
        "                    for img_index, img in enumerate(images):\n",
        "                        try:\n",
        "                            xref = img[0]\n",
        "                            base_image = pdf_document.extract_image(xref)\n",
        "                            image_bytes = base_image[\"image\"]\n",
        "\n",
        "                            # Конвертация байтов в изображение\n",
        "                            image = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "                            # OCR\n",
        "                            text = pytesseract.image_to_string(image, lang='rus+eng')\n",
        "                            if text.strip():\n",
        "                                documents.append(\n",
        "                                    Document(\n",
        "                                        page_content=text,\n",
        "                                        metadata={\n",
        "                                            \"source\": str(file_path),\n",
        "                                            \"type\": \"image_text\",\n",
        "                                            \"page\": page_num + 1,\n",
        "                                            \"image_index\": img_index\n",
        "                                        }\n",
        "                                    )\n",
        "                                )\n",
        "                        except Exception as e:\n",
        "                            self.logger.error(f\"Error processing image: {str(e)}\")\n",
        "                            continue\n",
        "\n",
        "            return documents\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting text from PDF: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def _split_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"Разбиение документов на чанки\"\"\"\n",
        "        try:\n",
        "            return self.text_splitter.split_documents(documents)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error splitting documents: {str(e)}\")\n",
        "            return documents\n",
        "\n",
        "    def _calculate_metadata(\n",
        "        self,\n",
        "        documents: List[Document],\n",
        "        file_path: Path,\n",
        "        processing_time: float\n",
        "    ) -> ProcessingMetadata:\n",
        "        \"\"\"Подсчет метаданных обработки\"\"\"\n",
        "        try:\n",
        "            word_count = sum(len(doc.page_content.split()) for doc in documents)\n",
        "            tables_count = sum(1 for doc in documents if doc.metadata.get('type') == 'table')\n",
        "            images_count = sum(1 for doc in documents if doc.metadata.get('type') == 'image_text')\n",
        "\n",
        "            return ProcessingMetadata(\n",
        "                document_type=file_path.suffix,\n",
        "                processing_date=datetime.datetime.now(),\n",
        "                chunks_count=len(documents),\n",
        "                tables_count=tables_count,\n",
        "                images_count=images_count,\n",
        "                word_count=word_count,\n",
        "                processing_time=processing_time,\n",
        "                hash=self._calculate_hash(file_path)\n",
        "            )\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error calculating metadata: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def process_document(self, file_path: Path) -> Tuple[List[Document], ProcessingMetadata]:\n",
        "        \"\"\"Обработка одного документа\"\"\"\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        try:\n",
        "            # Проверяем кэш\n",
        "            file_hash = self._calculate_hash(file_path)\n",
        "            cached_data = self._load_from_cache(file_hash)\n",
        "            if cached_data:\n",
        "                self.logger.info(f\"Using cached version for {file_path}\")\n",
        "                return cached_data\n",
        "\n",
        "            # Извлекаем текст в зависимости от типа документа\n",
        "            documents = []\n",
        "            if file_path.suffix.lower() == '.pdf':\n",
        "                documents = self._extract_text_from_pdf(file_path)\n",
        "            elif file_path.suffix.lower() in ['.docx', '.doc']:\n",
        "                documents = self._extract_text_from_docx(file_path)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported file type: {file_path.suffix}\")\n",
        "\n",
        "            # Разбиваем на чанки\n",
        "            chunked_documents = self._split_documents(documents)\n",
        "\n",
        "            # Считаем метаданные\n",
        "            processing_time = (datetime.datetime.now() - start_time).total_seconds()\n",
        "            metadata = self._calculate_metadata(chunked_documents, file_path, processing_time)\n",
        "\n",
        "            # Сохраняем в кэш\n",
        "            result = (chunked_documents, metadata)\n",
        "            self._save_to_cache(file_hash, result)\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing document {file_path}: {str(e)}\")\n",
        "            return [], None\n",
        "\n",
        "    def batch_process_documents(self, file_paths: List[Path]) -> List[Tuple[List[Document], ProcessingMetadata]]:\n",
        "        \"\"\"Пакетная обработка документов\"\"\"\n",
        "        results = []\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.batch_size) as executor:\n",
        "            future_to_path = {\n",
        "                executor.submit(self.process_document, path): path\n",
        "                for path in file_paths\n",
        "            }\n",
        "\n",
        "            for future in tqdm(\n",
        "                concurrent.futures.as_completed(future_to_path),\n",
        "                total=len(file_paths),\n",
        "                desc=\"Processing documents\"\n",
        "            ):\n",
        "                path = future_to_path[future]\n",
        "                try:\n",
        "                    documents, metadata = future.result()\n",
        "                    if documents and metadata:\n",
        "                        results.append((documents, metadata))\n",
        "                    else:\n",
        "                        self.logger.error(f\"Failed to process {path}\")\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error processing {path}: {str(e)}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def create_vector_store(\n",
        "        self,\n",
        "        documents: List[Document],\n",
        "        store_path: Optional[Path] = None\n",
        "    ) -> FAISS:\n",
        "        \"\"\"Создание векторного хранилища\"\"\"\n",
        "        try:\n",
        "            vector_store = FAISS.from_documents(documents, self.embeddings)\n",
        "            if store_path:\n",
        "                vector_store.save_local(str(store_path))\n",
        "            return vector_store\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error creating vector store: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def process(self, file_paths: Union[Path, List[Path]], **kwargs) -> Optional[FAISS]:\n",
        "        \"\"\"Основной метод обработки документов\"\"\"\n",
        "        try:\n",
        "            if isinstance(file_paths, Path):\n",
        "                file_paths = [file_paths]\n",
        "\n",
        "            # Обработка документов\n",
        "            processing_results = self.batch_process_documents(file_paths)\n",
        "\n",
        "            # Объединяем все документы\n",
        "            all_documents = []\n",
        "            for documents, metadata in processing_results:\n",
        "                all_documents.extend(documents)\n",
        "\n",
        "            # Создаем векторное хранилище\n",
        "            store_path = kwargs.get('store_path')\n",
        "            return self.create_vector_store(all_documents, store_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in document processing: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "class ResultsProcessor:\n",
        "    \"\"\"Процессор для обработки результатов и создания итогового CSV\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_dir: str = \"./results\",\n",
        "        answers_filename: str = \"answers.csv\",\n",
        "        enable_logging: bool = True\n",
        "    ):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.answers_filename = answers_filename\n",
        "        self.logger = logging.getLogger(self.__class__.__name__) if enable_logging else None\n",
        "\n",
        "    def process_qa_results(\n",
        "        self,\n",
        "        questions: List[str],\n",
        "        answers: List[str],\n",
        "        metadata: Optional[Dict[str, Any]] = None\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"Обработка результатов QA и создание DataFrame\"\"\"\n",
        "        try:\n",
        "            # Создаем DataFrame с вопросами и ответами\n",
        "            results_df = pd.DataFrame({\n",
        "                'question': questions,\n",
        "                'answer': answers\n",
        "            })\n",
        "\n",
        "            # Добавляем метаданные, если они есть\n",
        "            if metadata:\n",
        "                for key, value in metadata.items():\n",
        "                    results_df[key] = value\n",
        "\n",
        "            return results_df\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.logger:\n",
        "                self.logger.error(f\"Error processing QA results: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def save_results(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        output_path: Optional[Path] = None\n",
        "    ) -> Path:\n",
        "        \"\"\"Сохранение результатов в CSV\"\"\"\n",
        "        try:\n",
        "            # Определяем путь для сохранения\n",
        "            if output_path is None:\n",
        "                output_path = self.output_dir / self.answers_filename\n",
        "\n",
        "            # Оставляем только необходимые столбцы\n",
        "            final_df = df[['question', 'answer']].copy()\n",
        "\n",
        "            # Сохраняем в CSV\n",
        "            final_df.to_csv(output_path, index=False)\n",
        "\n",
        "            if self.logger:\n",
        "                self.logger.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "            return output_path\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.logger:\n",
        "                self.logger.error(f\"Error saving results: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def validate_csv_format(self, file_path: Path) -> bool:\n",
        "        \"\"\"Проверка формата CSV файла\"\"\"\n",
        "        try:\n",
        "            # Читаем CSV\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Проверяем наличие необходимых столбцов\n",
        "            required_columns = {'question', 'answer'}\n",
        "            if not required_columns.issubset(df.columns):\n",
        "                if self.logger:\n",
        "                    self.logger.error(f\"Missing required columns. Found: {df.columns}\")\n",
        "                return False\n",
        "\n",
        "            # Проверяем наличие данных\n",
        "            if df.empty:\n",
        "                if self.logger:\n",
        "                    self.logger.error(\"CSV file is empty\")\n",
        "                return False\n",
        "\n",
        "            # Проверяем наличие пропущенных значений\n",
        "            if df[list(required_columns)].isnull().any().any():\n",
        "                if self.logger:\n",
        "                    self.logger.error(\"CSV contains missing values\")\n",
        "                return False\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.logger:\n",
        "                self.logger.error(f\"Error validating CSV format: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "class TestDataProcessor:\n",
        "    \"\"\"Процессор для обработки тестового датасета\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        qa_agent: 'QAAgent',\n",
        "        results_processor: ResultsProcessor,\n",
        "        batch_size: int = 10\n",
        "    ):\n",
        "        self.qa_agent = qa_agent\n",
        "        self.results_processor = results_processor\n",
        "        self.batch_size = batch_size\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def process_test_questions(\n",
        "        self,\n",
        "        test_questions: List[str],\n",
        "        context: Optional[str] = None\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"Обработка тестовых вопросов\"\"\"\n",
        "        answers = []\n",
        "\n",
        "        # Обрабатываем вопросы батчами\n",
        "        for i in range(0, len(test_questions), self.batch_size):\n",
        "            batch_questions = test_questions[i:i + self.batch_size]\n",
        "\n",
        "            # Получаем ответы на батч вопросов\n",
        "            batch_answers = []\n",
        "            for question in batch_questions:\n",
        "                try:\n",
        "                    answer = self.qa_agent.process(\n",
        "                        question=question,\n",
        "                        context=context\n",
        "                    )\n",
        "                    batch_answers.append(answer)\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error processing question '{question}': {str(e)}\")\n",
        "                    batch_answers.append(\"Error processing question\")\n",
        "\n",
        "            answers.extend(batch_answers)\n",
        "\n",
        "            # Логируем прогресс\n",
        "            self.logger.info(f\"Processed {i + len(batch_questions)}/{len(test_questions)} questions\")\n",
        "\n",
        "        # Создаем DataFrame с результатами\n",
        "        return self.results_processor.process_qa_results(\n",
        "            questions=test_questions,\n",
        "            answers=answers,\n",
        "            metadata={\n",
        "                'processing_date': datetime.datetime.now().isoformat(),\n",
        "                'context_provided': context is not None\n",
        "            }\n",
        "        )\n",
        "\n",
        "class TestDatasetRunner:\n",
        "    \"\"\"Запуск обработки тестового датасета\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        qa_agent: 'QAAgent',\n",
        "        document_processor: 'DocumentProcessingAgent',\n",
        "        output_dir: str = \"./results\"\n",
        "    ):\n",
        "        self.qa_agent = qa_agent\n",
        "        self.document_processor = document_processor\n",
        "        self.results_processor = ResultsProcessor(output_dir=output_dir)\n",
        "        self.test_processor = TestDataProcessor(\n",
        "            qa_agent=qa_agent,\n",
        "            results_processor=self.results_processor\n",
        "        )\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        test_data_path: Path,\n",
        "        test_questions_path: Path,\n",
        "        context_docs_paths: Optional[List[Path]] = None\n",
        "    ) -> Path:\n",
        "        \"\"\"Запуск обработки тестового датасета\"\"\"\n",
        "        try:\n",
        "            # Загружаем тестовые вопросы\n",
        "            test_questions_df = pd.read_csv(test_questions_path)\n",
        "            test_questions = test_questions_df['question'].tolist()\n",
        "\n",
        "            # Обрабатываем контекстные документы, если они есть\n",
        "            context = None\n",
        "            if context_docs_paths:\n",
        "                processed_docs = self.document_processor.process(\n",
        "                    file_paths=context_docs_paths\n",
        "                )\n",
        "                if processed_docs:\n",
        "                    context = processed_docs\n",
        "\n",
        "            # Обрабатываем тестовые вопросы\n",
        "            results_df = self.test_processor.process_test_questions(\n",
        "                test_questions=test_questions,\n",
        "                context=context\n",
        "            )\n",
        "\n",
        "            # Сохраняем результаты\n",
        "            output_path = self.results_processor.save_results(results_df)\n",
        "\n",
        "            # Проверяем формат\n",
        "            if not self.results_processor.validate_csv_format(output_path):\n",
        "                raise ValueError(\"Generated CSV file has invalid format\")\n",
        "\n",
        "            self.logger.info(f\"Test dataset processing completed. Results saved to {output_path}\")\n",
        "            return output_path\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error running test dataset: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "class EcoQASystem:\n",
        "    \"\"\"Основная система эко-помощника\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Настройка окружения\n",
        "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "        os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n",
        "\n",
        "        # Инициализация логирования\n",
        "        self._setup_logging()\n",
        "\n",
        "        # Пути к данным\n",
        "        self.data_dir = Path(\"data\")\n",
        "        self.docs_dir = self.data_dir / \"documents\"\n",
        "        self.regulatory_dir = self.data_dir / \"regulatory\"\n",
        "        self.cache_dir = Path(\"cache\")\n",
        "\n",
        "        # Создание необходимых директорий\n",
        "        self._create_directories()\n",
        "\n",
        "        # Инициализация компонентов\n",
        "        self.qa_agent = None\n",
        "        self.doc_processor = None\n",
        "        self.vector_store = None\n",
        "\n",
        "        # Загрузка системы\n",
        "        self._initialize_system()\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        \"\"\"Настройка логирования\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler('eco_qa_system.log'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def _create_directories(self):\n",
        "        \"\"\"Создание необходимых директорий\"\"\"\n",
        "        for directory in [self.data_dir, self.docs_dir, self.regulatory_dir, self.cache_dir]:\n",
        "            directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Создаем пустой файл в regulatory_dir\n",
        "        if not any(self.regulatory_dir.iterdir()):\n",
        "            dummy_file = self.regulatory_dir / \"dummy.txt\"\n",
        "            dummy_file.write_text(\"Placeholder for regulatory documents\")\n",
        "\n",
        "    def _initialize_system(self):\n",
        "        \"\"\"Инициализация системы\"\"\"\n",
        "        try:\n",
        "            # Инициализация процессора документов\n",
        "            self.doc_processor = DocumentProcessor()\n",
        "\n",
        "            # Загрузка и обработка нормативных документов\n",
        "            self._load_regulatory_docs()\n",
        "\n",
        "            # Инициализация QA агента\n",
        "            self.qa_agent = EnhancedQAAgent(\n",
        "                model_name=\"IlyaGusev/saiga_mistral_7b\",\n",
        "                training_data_path=\"train.csv\",\n",
        "                vector_store=self.vector_store\n",
        "            )\n",
        "\n",
        "            self.logger.info(\"System initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error initializing system: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _load_regulatory_docs(self):\n",
        "        \"\"\"Загрузка нормативных документов\"\"\"\n",
        "        try:\n",
        "            regulatory_files = list(self.regulatory_dir.glob(\"*.pdf\")) + \\\n",
        "                             list(self.regulatory_dir.glob(\"*.docx\"))\n",
        "\n",
        "            if not regulatory_files:\n",
        "                self.logger.warning(\"No regulatory documents found!\")\n",
        "                return\n",
        "\n",
        "            # Обработка документов\n",
        "            docs = []\n",
        "            for file in tqdm(regulatory_files, desc=\"Processing regulatory docs\"):\n",
        "                if file.suffix.lower() == '.pdf':\n",
        "                    doc = PDFDocument(str(file))\n",
        "                else:\n",
        "                    doc = DocxDocument(str(file))\n",
        "\n",
        "                doc.load()\n",
        "                docs.extend(self.doc_processor.process_document(doc))\n",
        "\n",
        "            # Создание векторного хранилища\n",
        "            self.vector_store = FAISS.from_documents(docs, self.embeddings)\n",
        "            self.logger.info(f\"Loaded {len(regulatory_files)} regulatory documents\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading regulatory documents: {e}\")\n",
        "            raise\n",
        "\n",
        "    def load_document(self, file_path: str) -> bool:\n",
        "        \"\"\"Загрузка нового документа\"\"\"\n",
        "        try:\n",
        "            file_path = Path(file_path)\n",
        "            if not file_path.exists():\n",
        "                raise FileNotFoundError(f\"File {file_path} not found\")\n",
        "\n",
        "            # Определяем тип документа и загружаем\n",
        "            if file_path.suffix.lower() == '.pdf':\n",
        "                doc = PDFDocument(str(file_path))\n",
        "            elif file_path.suffix.lower() in ['.docx', '.doc']:\n",
        "                doc = DocxDocument(str(file_path))\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported file type: {file_path.suffix}\")\n",
        "\n",
        "            # Загружаем и обрабатываем документ\n",
        "            doc.load()\n",
        "            new_docs = self.doc_processor.process_document(doc)\n",
        "\n",
        "            # Добавляем в векторное хранилище\n",
        "            if self.vector_store is None:\n",
        "                self.vector_store = FAISS.from_documents(new_docs, self.embeddings)\n",
        "            else:\n",
        "                self.vector_store.add_documents(new_docs)\n",
        "\n",
        "            self.logger.info(f\"Successfully loaded document: {file_path}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading document {file_path}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def ask_question(self, question: str) -> str:\n",
        "        \"\"\"Задать вопрос системе\"\"\"\n",
        "        try:\n",
        "            if self.qa_agent is None:\n",
        "                raise ValueError(\"QA system not initialized\")\n",
        "\n",
        "            answer = self.qa_agent.process_question(question)\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing question: {e}\")\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    def process_questions_file(self, file_path: str, output_path: Optional[str] = None) -> pd.DataFrame:\n",
        "        \"\"\"Обработка файла с вопросами\"\"\"\n",
        "        try:\n",
        "            # Загружаем вопросы\n",
        "            questions_df = pd.read_csv(file_path)\n",
        "\n",
        "            # Обрабатываем каждый вопрос\n",
        "            answers = []\n",
        "            for question in tqdm(questions_df['question'], desc=\"Processing questions\"):\n",
        "                answer = self.ask_question(question)\n",
        "                answers.append(answer)\n",
        "\n",
        "            # Создаем DataFrame с результатами\n",
        "            results_df = pd.DataFrame({\n",
        "                'question': questions_df['question'],\n",
        "                'answer': answers\n",
        "            })\n",
        "\n",
        "            # Сохраняем результаты\n",
        "            if output_path:\n",
        "                results_df.to_csv(output_path, index=False)\n",
        "                self.logger.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "            return results_df\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing questions file: {e}\")\n",
        "            raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        print(\"Initializing Eco QA System...\")\n",
        "        system = EcoQASystem()\n",
        "\n",
        "        while True:\n",
        "            print(\"\\nOptions:\")\n",
        "            print(\"1. Load document\")\n",
        "            print(\"2. Ask question\")\n",
        "            print(\"3. Process questions from file\")\n",
        "            print(\"4. Exit\")\n",
        "\n",
        "            choice = input(\"\\nEnter your choice (1-4): \")\n",
        "\n",
        "            if choice == \"1\":\n",
        "                file_path = input(\"Enter document path: \")\n",
        "                if system.load_document(file_path):\n",
        "                    print(\"Document loaded successfully\")\n",
        "                else:\n",
        "                    print(\"Failed to load document\")\n",
        "\n",
        "            elif choice == \"2\":\n",
        "                question = input(\"Enter your question: \")\n",
        "                answer = system.ask_question(question)\n",
        "                print(f\"\\nAnswer: {answer}\")\n",
        "\n",
        "            elif choice == \"3\":\n",
        "                input_file = input(\"Enter questions file path: \")\n",
        "                output_file = input(\"Enter output file path (or press Enter for default): \")\n",
        "                if not output_file:\n",
        "                    output_file = \"answers.csv\"\n",
        "\n",
        "                results = system.process_questions_file(input_file, output_file)\n",
        "                print(f\"Processed {len(results)} questions\")\n",
        "\n",
        "            elif choice == \"4\":\n",
        "                print(\"Exiting...\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                print(\"Invalid choice\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RrmAvKVofBuo"
      },
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cbd698f2ee5a408aac2f3dc4d8d8fdf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23bcb18458874792bb0c67a9d4a3e609",
              "IPY_MODEL_48a65cdf783b45b89ee6007b3303baa4",
              "IPY_MODEL_e3aa7117b8c24a8a935df4be99c52a2a"
            ],
            "layout": "IPY_MODEL_37c593dff5394a659629851e79db995e"
          }
        },
        "23bcb18458874792bb0c67a9d4a3e609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_568eafc843a14e9982ef66faaa943b87",
            "placeholder": "​",
            "style": "IPY_MODEL_7d276087fc8d4f1fa95f043e44fff651",
            "value": "Loading checkpoint shards:  50%"
          }
        },
        "48a65cdf783b45b89ee6007b3303baa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94cbdd0ac68940f2bbd43ae93838889b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dbe923df713649afa635c3ca993049b0",
            "value": 1
          }
        },
        "e3aa7117b8c24a8a935df4be99c52a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66bbd0aee079401c81daa1ed8348b957",
            "placeholder": "​",
            "style": "IPY_MODEL_6ce6ae18829440879239f5cea7237062",
            "value": " 1/2 [00:51&lt;00:51, 51.96s/it]"
          }
        },
        "37c593dff5394a659629851e79db995e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "568eafc843a14e9982ef66faaa943b87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d276087fc8d4f1fa95f043e44fff651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94cbdd0ac68940f2bbd43ae93838889b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbe923df713649afa635c3ca993049b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66bbd0aee079401c81daa1ed8348b957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ce6ae18829440879239f5cea7237062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}